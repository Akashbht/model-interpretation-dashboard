#!/usr/bin/env python3
"""
Simple test script to verify the backend functionality
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Test imports
try:
    from models.model_manager import ModelManager
    from evaluation.benchmark_runner import BenchmarkRunner
    from evaluation.leaderboard import Leaderboard
    from evaluation.metrics import MetricsCalculator
    print("âœ… All imports successful")
except ImportError as e:
    print(f"âŒ Import error: {e}")
    sys.exit(1)

# Test ModelManager
print("\nğŸ§ª Testing ModelManager...")
try:
    model_manager = ModelManager()
    available_models = model_manager.get_available_models()
    print(f"âœ… ModelManager works. Found {len(available_models)} models")
    for model in available_models:
        print(f"   - {model.get('name', 'Unknown')} ({model.get('provider', 'Unknown')})")
except Exception as e:
    print(f"âŒ ModelManager error: {e}")

# Test BenchmarkRunner
print("\nğŸ§ª Testing BenchmarkRunner...")
try:
    benchmark_runner = BenchmarkRunner(model_manager)
    print("âœ… BenchmarkRunner initialized successfully")
except Exception as e:
    print(f"âŒ BenchmarkRunner error: {e}")

# Test Leaderboard
print("\nğŸ§ª Testing Leaderboard...")
try:
    leaderboard = Leaderboard()
    rankings = leaderboard.get_rankings()
    print(f"âœ… Leaderboard works. Found {len(rankings.get('rankings', []))} ranked models")
except Exception as e:
    print(f"âŒ Leaderboard error: {e}")

# Test MetricsCalculator
print("\nğŸ§ª Testing MetricsCalculator...")
try:
    calc = MetricsCalculator()
    
    # Test latency score
    latency_score = calc.calculate_latency_score(2.5)
    print(f"âœ… Latency score for 2.5s: {latency_score:.1f}")
    
    # Test cost score
    cost_score = calc.calculate_cost_score(0.02)
    print(f"âœ… Cost score for $0.02: {cost_score:.1f}")
    
    # Test quality score
    quality_score = calc.calculate_quality_score("This is a good response to the prompt.")
    print(f"âœ… Quality score for sample text: {quality_score:.1f}")
    
except Exception as e:
    print(f"âŒ MetricsCalculator error: {e}")

print("\nğŸ‰ Backend test complete!")